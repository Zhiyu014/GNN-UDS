astlingen:
  env: astlingen
  directed: False
  length: 0
  order: 1
  graph_base: 0
  setting_duration: 5
  act: 'False'
  mac: False
  dec: False

  model_dir: ./model/astlingen/
  epsilon: -1.0

  data_dir: ./envs/data/astlingen/
  if_flood: False
  horizon: 10 # horizon length
  conv: GATconv
  use_pred: False  # if use prediction runoff as states
  net_dim: 128 # dimension of hidden layers
  n_layer: 3 # number of network layers
  conv_dim: 128 # dimension of hidden layers
  n_sp_layer: 3 # number of network layers
  activation: relu # if use dueling layer
  vnet: False # if use state value network

  # Arguments for the training
  agent: SAC  # agent name
  train: False
  episodes: 5000
  repeats: 5
  gamma: 0.98 # discouted rate
  norm: False # normalize the reward
  scale: 1.0 # scale the reward
  batch_size: 128 # batch size of the training data
  limit: 22 # maximum capacity of the replay buffer
  act_lr: 0.0001 # learning rate for actor
  cri_lr: 0.001 # learning rate for critic
  update_interval: 0.005 # update the target network per update_interval
  epsilon_decay: 0.9996 # epsilon decay rate for deterministic q learning
  value_tau: 0.0 # value running average tau
  model_based: False # if use model-based sampling
  sample_gap: 0 # sample data with swmm per sample gap
  start_gap: 100 # start updating agent after start gap
  save_gap: 100 # save the agent & state_norm per save_gap
  agent_dir: ./agent/astlingen/ # the working directory
  load_agent: False # if load the current model

  # Arguments for the testing
  test: False
  rain_dir: ./envs/config/  # rainfall info dir
  rain_suffix:  # rain_suffix
  rain_num: 50 # exploration events
  processes: 1
  eval_gap: 10 # evaluate the agent per eval_gap
  control_interval: 5
  result_dir: ./results/astlingen/ # the result directory

chaohu:
  env: chaohu
  directed: False
  length: 0
  order: 1
  graph_base: 0
  setting_duration: 10
  act: 'False'
  mac: False
  dec: False

  model_dir: ./model/chaohu/
  epsilon: -1.0

  data_dir: ./envs/data/chaohu/
  if_flood: False
  horizon: 10 # horizon length
  conv: GATconv
  use_pred: False  # if use prediction runoff as states
  net_dim: 128 # dimension of hidden layers
  n_layer: 3 # number of network layers
  conv_dim: 128 # dimension of hidden layers
  n_sp_layer: 3 # number of network layers
  activation: relu # if use dueling layer
  vnet: False # if use state value network

  # Arguments for the training
  agent: SAC  # agent name
  train: False
  episodes: 5000
  repeats: 10
  gamma: 0.95 # discouted rate
  norm: False # normalize the reward
  scale: 1.0 # scale the reward
  limit: 22 # maximum capacity of the replay buffer
  batch_size: 128 # batch size of the training data
  act_lr: 0.0001 # learning rate for actor
  cri_lr: 0.001 # learning rate for critic
  update_interval: 0.005 # update the target network per update_interval
  epsilon_decay: 0.9996 # epsilon decay rate for deterministic q learning
  value_tau: 0.0 # value running average tau
  model_based: False # if use model-based sampling
  sample_gap: 0 # sample data with swmm per sample gap
  start_gap: 100 # start updating agent after start gap
  save_gap: 100 # save the agent & state_norm per save_gap
  agent_dir: ./agent/chaohu/ # the working directory
  load_agent: False # if load the current model

  # Arguments for the testing
  test: False
  rain_dir: ./envs/config/  # rainfall info dir
  rain_suffix:  # rain_suffix
  rain_num: 50 # exploration events
  processes: 1
  eval_gap: 10 # evaluate the agent per eval_gap
  control_interval: 10
  result_dir: ./results/chaohu/ # the result directory
